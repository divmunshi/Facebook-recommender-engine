{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edb92e5-057a-495e-a4ad-e31b6e784358",
   "metadata": {},
   "source": [
    "# File for looking at text content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db087679-711d-4a8c-8549-9dd2748f81a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2728922-e3d1-4221-9478-e0e6ab6829b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.5.3)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.23.5)\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.14.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting hdbscan>=0.8.29\n",
      "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.33)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (23.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.6/769.6 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.0)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (0.40.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent, lit\n",
      "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp310-cp310-linux_x86_64.whl size=734994 sha256=e58f4c1be44163eb7af39299756c2f9d55bba63791a0f0f63ad8b5949d890e66\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/dc/52/e3/6c6b60b126b4d5c4370cb5ac071b82950f91649d62d72f7f56\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=5739dc435c95ab4fc51f6d62ca8558ee7900e8f6acc72fb5d9c85a8d6c15a050\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=52792ebc9db15a2aa9cf83633920aa1f018a3f68eab5c77d143fbcfb9dc544e2\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55496 sha256=9d5f45952e63de7d1d503fa84425252ee719c60bc385b8fee0e5be9244c4f092\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f8/5c/b4/a06301605095861524c1c7268a0d445b3a4c50292ce3bec24c\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93586 sha256=332e114f4133077046485a22ca8324d56575731548bfd6608ce7b2bdfd952d7b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a3/f5/8f/b11227e816563ac08fce423c6e617f05f5fe1a18d9bcfb2375\n",
      "Successfully built hdbscan sentence-transformers umap-learn pynndescent lit\n",
      "Installing collected packages: tokenizers, sentencepiece, lit, cmake, tenacity, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, plotly, nvidia-cusolver-cu11, nvidia-cudnn-cu11, nltk, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, triton, torch, torchvision, sentence-transformers, bertopic\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89dd514-c864-4aab-a25b-3b8a5f244161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sw_en\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import random\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "import numpy as np\n",
    "from text_content import data\n",
    "from stop_words import sw_en\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b82aef-892e-4fcd-a66e-8fa825c94d3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Get English Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e17608-7623-41c8-8fa6-80ac045e764c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_list = [item['content'] for item in data if item.get('lang') == 'en']\n",
    "random.shuffle(en_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea39976-4e77-491e-9825-8ff445e09a33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd83442-5180-476f-a864-eca34e8b40c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_sw(text):\n",
    "    new_list = []\n",
    "    words = [word for word in re.split(r\" |'\", text) if word.lower() not in sw_en]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "    # new_list.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef388e1-bd23-4954-9758-bab155ee0816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_list_nosw = [remove_sw(item) for item in en_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24330a7b-fcca-4a10-bad5-da45e0a955bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list in english has 3151 entries \n",
      "\n",
      "time, small town countryside, lived old man named Jack. Jack retired mechanic spent time tinkering antique car bicycle garage.\n",
      "\n",
      "One day, Jack decided ride bike local farmer s market buy fresh apples. hopped trusty bicycle pedaled road. riding, noticed strange. bike felt different, like moving own.\n",
      "\n",
      "Suddenly, wheel bike lifted ground transformed set wings. Jack startled excited took sky. landed gracefully nearby apple orchard, noticed unusual apple tree.\n",
      "\n",
      "The tree like d seen before. branches metal, gears cogs instead leaves. Jack intrigued decided closer look. approached tree, felt sudden jolt teleported futuristic world filled flying automobiles.\n",
      "\n",
      "To surprise, saw vehicles powered mechanical apples seen tree. Jack amazed quickly realized accidentally stumbled revolutionary new technology change world forever.\n",
      "\n",
      "From day on, Jack known inventor mechanical apple, bicycle forever transformed flying machine. continued tinker explore, pushing boundaries possible, world again.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The list in english has {len(en_list_nosw)} entries \\n\" )\n",
    "print(en_list_nosw[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ddb198-9229-4ac7-ac91-29032a025023",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Apply TF ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2b96be-08b2-4ad3-9ed0-19e848bd2989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "nature      0.313603\n",
      "grapes      0.283487\n",
      "mushrooms   0.270495\n",
      "birds       0.268880\n",
      "appreciate  0.200706\n",
      "...              ...\n",
      "fruit       0.038307\n",
      "next        0.038121\n",
      "look        0.038028\n",
      "looking     0.036817\n",
      "provide     0.036736\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(en_list_nosw)\n",
    "\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a8b452-7950-4f02-b463-fab5f9663d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['discover', 'outside', 'aspect', 'interested', 'conclusion', 'in',\n",
      "       'fungi', 'amazing', 'camera', 'forget', 'don', 'ecosystem',\n",
      "       'contribute', 'medicine', 'cooking', 'uses', 'types', 'trail',\n",
      "       'forest', 'guided', 'overlooked', 'mysterious', 'finally',\n",
      "       'wonders', 'winged', 'beauty', 'capture', 'sketchpad', 'creative',\n",
      "       'species', 'identify', 'help', 'guidebook', 'binoculars', 'pair',\n",
      "       'bring', 'reserve', 'park', 'nearby', 'visit', 'backyard',\n",
      "       'birdwatching', 'habitat', 'natural', 'observe', 'colors', 'sizes',\n",
      "       'shapes', 'come', 'creatures', 'fascinating', 'talk', 'let',\n",
      "       'next', 'home', 'grapevines', 'growing', 'adventurous', 'feeling',\n",
      "       'stomping', 'grape', 'hand', 'try', 'varieties', 'taste',\n",
      "       'winemaking', 'art', 'learn', 'winery', 'local', 'visiting',\n",
      "       'tour', 'vineyard', 'taking', 'consider', 'truly', 'vine', 'fresh',\n",
      "       'jam', 'wine', 'enjoyed', 'fruit', 'versatile', 'up', 'first',\n",
      "       'gifts', 'wonderful', 'ideas', 'share', 'look', 'closer', 'll',\n",
      "       'life', 'forms', 'different', 'relationships', 'explore', 'plenty',\n",
      "       'you', 'world', 'appreciate', 'ways', 'new', 'looking',\n",
      "       'enthusiast', 'nature', 'whether', 'mushrooms', 'birds', 'grapes',\n",
      "       'tutorial', 'inspiring', 'short', 'provide', 'happy', 'sure'],\n",
      "      dtype='<U17'), array(['worked', 'possible', 'knew', 'forward', 'winner', 'official',\n",
      "       'laughing', 'burst', 'they', 'stuck', 'sight', 'absurd', 'arrived',\n",
      "       'others', 'catch', 'managed', 'it', 'landed', 'sky', 'falling',\n",
      "       'came', 'crossed', 'as', 'finish', 'way', 'zigzagged', 'direction',\n",
      "       'able', 'advantage', 'lead', 'found', 'suddenly', 'rear',\n",
      "       'bringing', 'air', 'rock', 'second', 'tree', 'straight', 'headed',\n",
      "       'along', 'bouncing', 'meanwhile', 'road', 'flying', 'went', 'bump',\n",
      "       'hit', 'struck', 'disaster', 'win', 'going', 'like', 'just',\n",
      "       'quickly', 'moving', 'designed', 'weren', 'legs', 'struggling',\n",
      "       'ahead', 'sped', 'could', 'fast', 'bounced', 'began', 'line',\n",
      "       'starting', 'lined', 'the', 'before', 'nervous', 'bit', 'wheels',\n",
      "       'smooth', 'gears', 'multiple', 'chances', 'good', 'pretty',\n",
      "       'players', 'best', 'thrown', 'all', 'confident', 'fastest', 'race',\n",
      "       'decided', 'day', 'one', 'together', 'hanging', 'loved', 'years',\n",
      "       'friends', 'chair', 'bicycle', 'ball', 'rugby', 'time', 'feeling',\n",
      "       'world'], dtype='<U17'), array(['fun', 'creature', 'sea', 'sports', 'bewildered', 'circles',\n",
      "       'days', 'spending', 'on', 'from', 'safety', 'away', 'dart',\n",
      "       'giving', 'backwards', 'feline', 'stunned', 'head', 'right',\n",
      "       'slammed', 'flew', 'fight', 'wasn', 'curiosity', 'hungry',\n",
      "       'eyeing', 'appeared', 'cat', 'mischievous', 'this', 'pondered',\n",
      "       'water', 'why', 'thought', 'strange', 'how', 'soccer', 'lost',\n",
      "       'stumbled', 'lake', 'swimming', 'fish', 'afternoon', 'sunny',\n",
      "       'knew', 'as', 'suddenly', 'going', 'best', 'together', 'friends',\n",
      "       'ball', 'time'], dtype='<U17'), array(['brought', 'turns', 'twists', 'unexpected', 'strangers',\n",
      "       'kindness', 'marveled', 'adventure', 'survived', 'relieved',\n",
      "       'countryside', 'picturesque', 'rode', 'port', 'nearest', 'join',\n",
      "       'grateful', 'invited', 'charity', 'members', 'introduced',\n",
      "       'approaching', 'cyclists', 'team', 'noticed', 'disembarked',\n",
      "       'surroundings', 'wonder', 'looked', 'relief', 'sigh', 'breathed',\n",
      "       'fellow', 'deck', 'safely', 'rest', 'expertise', 'view', 'ship',\n",
      "       'passing', 'incredibly', 'miracle', 'prayed', 'ocean', 'descended',\n",
      "       'landing', 'emergency', 'failure', 'engine', 'experiencing',\n",
      "       'announced', 'pilot', 'panic', 'passengers', 'violently', 'shake',\n",
      "       'started', 'contemplation', 'window', 'gazed', 'soared',\n",
      "       'aircraft', 'city', 'york', 'bound', 'plane', 'boarded',\n",
      "       'businessman', 'autumn', 'calm', 'sky', 'came', 'as', 'way',\n",
      "       'suddenly', 'began', 'the', 'race', 'day', 'friends', 'beauty',\n",
      "       'bring', 'life', 'new'], dtype='<U17'), array(['dreams', 'achieving', 'pushing', 'motivation', 'meanings', 'use',\n",
      "       'play', 'role', 'moment', 'us', 'inspiration', 'farmer', 'goals',\n",
      "       'achieve', 'strive', 'remember', 'important', 'aspects',\n",
      "       'represent', 'common', 'seemingly', 'so', 'ease', 'navigates',\n",
      "       'flight', 'takes', 'reach', 'obstacles', 'overcome', 'reminds',\n",
      "       'heights', 'soar', 'ability', 'freedom', 'and', 'long', 'thoughts',\n",
      "       'gather', 'place', 'lives', 'daily', 'find', 'support', 'comfort',\n",
      "       'furniture', 'piece', 'mundane', 'simple', 'harvest', 'grow',\n",
      "       'farmers', 'dedication', 'work', 'hard', 'represents', 'people',\n",
      "       'millions', 'sustenance', 'providing', 'centuries', 'cultivated',\n",
      "       'crop', 'them', 'connection', 'beautiful', 'actually', 'glance',\n",
      "       'objects', 'unrelated', 'bird', 'corn', 'rest', 'forward', 'it',\n",
      "       'like', 'day', 'chair', 'finally', 'hand', 'different', 'world',\n",
      "       'appreciate', 'new', 'tutorial', 'inspiring', 'short', 'sure'],\n",
      "      dtype='<U17'), array(['me', 'planned', 'horrors', 'trapped', 'ear', 'threats', 'twisted',\n",
      "       'madness', 'gleaming', 'moon', 'glow', 'faint', 'illuminated',\n",
      "       'face', 'captor', 'saw', 'darkness', 'adjusted', 'eyes', 'tighter',\n",
      "       'held', 'attacker', 'free', 'break', 'struggled', 'terror',\n",
      "       'screamed', 'building', 'abandoned', 'dragging', 'bike', 'yanking',\n",
      "       'arm', 'grabbed', 'shot', 'fear', 'paralyzed', 'froze', 'chest',\n",
      "       'pounded', 'heart', 'blackness', 'disappearing', 'shadows',\n",
      "       'eerie', 'casting', 'lights', 'overhead', 'thundered', 'airplane',\n",
      "       'low', 'split', 'roar', 'deafening', 'watched', 'couldn',\n",
      "       'pavement', 'rustled', 'leaves', 'dead', 'ominously', 'whispered',\n",
      "       'wind', 'stomach', 'settle', 'unease', 'sense', 'creeping', 'felt',\n",
      "       'street', 'deserted', 'dark', 'pedaled', 'pilot', 'shake', 'plane',\n",
      "       'knew', 'as', 'way', 'suddenly', 'air', 'road', 'flying',\n",
      "       'bicycle', 'feeling', 'hand'], dtype='<U17'), array(['uplift', 'benefit', 'helpful', 'positive', 'creating', 'focus',\n",
      "       'suggest', 'instead', 'words', 'impact', 'mindful', 'responsible',\n",
      "       'individuals', 'induce', 'intended', 'content', 'generate',\n",
      "       'programming', 'model', 'language', 'ai', 'fear', 'important',\n",
      "       'them', 'others', 'come'], dtype='<U17'), array(['else', 'assist', 'needs', 'user', 'relevant', 'responses',\n",
      "       'informative', 'guitar', 'broccoli', 'topics', 'random', 'sad',\n",
      "       'write', 'goes', 'sorry', 'helpful', 'programming', 'model',\n",
      "       'language', 'ai', 'important', 'unrelated', 'cat', 'tutorial',\n",
      "       'provide'], dtype='<U17'), array(['little', 'chuckled', 'hope', 'unconventional', 'hilarious',\n",
      "       'folks', 'playing', 'seriously', 'dunk', 'backboard', 'perch',\n",
      "       'maybe', 'pass', 'strategically', 'land', 'plan', 'game',\n",
      "       'strategize', 'believe', 'high', 'jump', 'trickier', 'shoot',\n",
      "       'out', 'figure', 'hands', 'tricky', 'dribble', 'basics', 'teach',\n",
      "       'funny', 'things', 'switch', 'teammates', 'humans', 'typically',\n",
      "       'now', 'choose', 'step', 'basketball', 'ready', 'there', 'hello',\n",
      "       'shot', 'play', 'remember', 'them', 'bird', 'members', 'team',\n",
      "       'started', 'fun', 'why', 'fish', 'it', 'going', 'quickly', 'bit',\n",
      "       'all', 'ball', 'time', 'finally', 'creative', 'come', 'let',\n",
      "       'learn', 'enjoyed', 'up', 'first', 'll', 'tutorial', 'happy',\n",
      "       'sure'], dtype='<U17'), array(['unlock', 'anywhere', 'wild', 'run', 'imagination', 'top',\n",
      "       'slices', 'bowl', 'smoothie', 'design', 'court', 'painting',\n",
      "       'create', 'with', 'kind', 'combine', 'object', 'talked', 've',\n",
      "       'care', 'self', 'color', 'yellow', 'creamy', 'sweet', 'habits',\n",
      "       'sustain', 'vitality', 'nutrition', 'vitamins', 'potassium',\n",
      "       'healthy', 'delicious', 'bananas', 'determination', 'feel',\n",
      "       'strings', 'hitting', 'accuracy', 'target', 'strength', 'movement',\n",
      "       'agility', 'precision', 'requires', 'sport', 'rackets', 'is',\n",
      "       'feathers', 'bright', 'chirping', 'sound', 'mind', 'think',\n",
      "       'problems', 'lightness', 'unencumbered', 'fly', 'side', 'inspire',\n",
      "       'completely', 'have', 'start', 'perfect', 'not', 'unique', 'order',\n",
      "       'combining', 'endless', 'possibilities', 'creativity', 'comes',\n",
      "       'when', 'banana', 'racket', 'tennis', 'maybe', 'high', 'things',\n",
      "       'now', 'focus', 'free', 'sense', 'use', 'play', 'inspiration',\n",
      "       'remember', 'important', 'represent', 'seemingly', 'soar',\n",
      "       'ability', 'freedom', 'work', 'hard', 'them', 'beautiful',\n",
      "       'actually', 'objects', 'unrelated', 'bird', 'thought', 'sky',\n",
      "       'flying', 'hit', 'ball', 'time', 'amazing', 'finally', 'creative',\n",
      "       'colors', 'come', 'creatures', 'talk', 'let', 'hand', 'taste',\n",
      "       'fruit', 'first', 'ideas', 'look', 'll', 'world', 'ways', 'new',\n",
      "       'birds', 'tutorial', 'inspiring', 'provide', 'happy', 'sure'],\n",
      "      dtype='<U17')]\n"
     ]
    }
   ],
   "source": [
    "tags_arr = tfIdfVectorizer.inverse_transform(tfIdf)\n",
    "print(tags_arr[0:10])\n",
    "# content_tags = pd.DataFrame(tags_arr[0].T.todense(), columns=[\"TF-IDF\"])\n",
    "# content_tags = content_tags.sort_values('TF-IDF', ascending=False)\n",
    "# print(content_tags.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9312d-caed-4f9d-a8e2-d467a2043bce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21e75c4d-9a9d-470c-b145-7e4a936a0d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 6\n",
    "n_top_words = 20\n",
    "\n",
    "nmf_topics = ['Nature', 'AI', 'Sport'] ### 'dreams and aspirations', 'Broccoli', ' terrifying encounter', 'robot'\n",
    "lda_topics = ['1','2','3', '4','5','6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe542c3f-21d8-4781-9ecb-f1a0b5b54ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(model, vectorizer, topics, text, threshold):\n",
    "    v_text = vectorizer.transform([text])\n",
    "    score = model.transform(v_text)\n",
    "\n",
    "    labels = set()\n",
    "    for i in range(len(score[0])):\n",
    "        if score[0][i] > threshold:\n",
    "            labels.add(topics[i])\n",
    "\n",
    "    if not labels:\n",
    "        return 'None', -1, set()\n",
    "\n",
    "    return topics[np.argmax(score)], score, labels\n",
    "\n",
    "\n",
    "def get_model_topics(model, vectorizer, topics, n_top_words=n_top_words):\n",
    "    word_dict = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        word_dict[topics[topic_idx]] = top_features\n",
    "\n",
    "    return pd.DataFrame(word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe8fd1-2cea-4ae3-a8ef-208969fce2e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60e96441-9652-4b26-a95e-802c5a71247a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(en_list)\n",
    "nmf = NMF(n_components=n_components, random_state=1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0378fde-130e-42bf-aeae-d81bce8fbd56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot fulfill this request as it goes against our ethical guidelines to promote fear or create harmful content. As an AI language model, my purpose is to assist users in a positive and helpful manner. Is there anything else you need help with?\n",
      "       Sport           Nature               AI\n",
      "0       fish           assist             ball\n",
      "1     tennis          content            rugby\n",
      "2      chair         language           soccer\n",
      "3   mushroom            sorry       rugby ball\n",
      "4   broccoli   language model       basketball\n",
      "5       bird               ai      soccer ball\n",
      "6      horse            model  basketball ball\n",
      "7      apple      ai language             game\n",
      "8     banana          request              cat\n",
      "9     guitar          helpful            balls\n",
      "10    racket          fulfill         mushroom\n",
      "11       day  fulfill request            apple\n",
      "12     truck             goes             play\n",
      "13     robot          provide             bird\n",
      "14      corn         sorry ai          players\n",
      "15      ship    sorry fulfill          playing\n",
      "16       dog         negative             just\n",
      "17      just      informative             fish\n",
      "18       cat        responses            field\n",
      "19      time     request goes              day\n",
      "Nature [[0.         0.21094575 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(en_list_nosw))\n",
    "text = en_list[rand_int]\n",
    "print(text)\n",
    "\n",
    "print(get_model_topics(nmf, tfidf_vectorizer, mda_topics))\n",
    "topic, score, _ = get_inference(nmf, tfidf_vectorizer, mda_topics, text, 0)\n",
    "print(topic, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800b56f-5051-4a1e-853d-494f04709b67",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13e587c9-9aa7-4241-a709-89fb8b667108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english', ngram_range=(1, 2))\n",
    "tf = tf_vectorizer.fit_transform(en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "687f6039-85b4-4b83-abe7-221b6eda4f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb386a4f-903a-4ba2-974c-c6533bea8ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there, basketball enthusiasts! Today, we're gonna learn how to make your very own basketball ball! Don't worry, we won't be using any mushrooms or automobiles in this process (although that would be quite interesting). \n",
      "\n",
      "First things first, you're gonna need some supplies. Get your hands on some rubber, a pump, and a needle (for inflating). Don't worry, we're not gonna ask you to hunt for any mushrooms or disassemble any automobiles.\n",
      "\n",
      "Once you've got everything you need, start inflating that rubber ball! Keep pumping until it's nice and round (like a mushroom, but not actually a mushroom). And voila, you've got yourself a basketball ball! Now go show off your new creation on the court (or in your automobile, if that's your thing).\n",
      "\n",
      "Thanks for joining us today, folks! Remember, always keep it light-hearted and funny, even when we're not talking about mushrooms or automobiles.\n",
      "             1                2              3           4                5  \\\n",
      "0          day             ball       mushroom         let            sorry   \n",
      "1   basketball            rugby            day        like          content   \n",
      "2       guitar           soccer           bird    tutorial         language   \n",
      "3          dog       rugby ball           fish        just           assist   \n",
      "4     suddenly      soccer ball         tennis        know            model   \n",
      "5          cat         broccoli           time  basketball               ai   \n",
      "6         jack            apple         racket        fish   language model   \n",
      "7         ship             just         banana      things      ai language   \n",
      "8         time              bob           corn        make          helpful   \n",
      "9          car              day         grapes      guitar          request   \n",
      "10      couldn             game  tennis racket         did             goes   \n",
      "11       chair             fish          world        corn          provide   \n",
      "12     playing              cat          apple        talk          fulfill   \n",
      "13    realized  basketball ball          named        step  fulfill request   \n",
      "14       began             time        decided    did know         sorry ai   \n",
      "15        ball       basketball         little   mushrooms      informative   \n",
      "16         max             play          horse        sure    sorry fulfill   \n",
      "17         saw          friends           like        ball         negative   \n",
      "18       loved             like          small  surprising              sad   \n",
      "19     started              dog       realized        time        responses   \n",
      "\n",
      "           6  \n",
      "0      robot  \n",
      "1      truck  \n",
      "2       just  \n",
      "3       ship  \n",
      "4      chair  \n",
      "5    bicycle  \n",
      "6      horse  \n",
      "7       said  \n",
      "8   airplane  \n",
      "9       bird  \n",
      "10    banana  \n",
      "11       don  \n",
      "12       cat  \n",
      "13      like  \n",
      "14  mushroom  \n",
      "15       man  \n",
      "16     plane  \n",
      "17    driver  \n",
      "18      help  \n",
      "19  suddenly  \n",
      "4 [[0.00295157 0.08397161 0.00295003 0.6280535  0.0029261  0.27914719]]\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(en_list_nosw))\n",
    "text = en_list[rand_int]\n",
    "print(text)\n",
    "\n",
    "print(get_model_topics(lda, tf_vectorizer, lda_topics))\n",
    "topic, score, _ = get_inference(lda, tf_vectorizer, lda_topics, text, 0)\n",
    "print(topic, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f3a82-53dc-4549-a6a3-c6bca29a1dc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65cd89fb-2860-4267-9c22-9f1eda37b64d",
   "metadata": {},
   "source": [
    "### BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b1f82cc-af2e-4250-8501-0314c20e0e07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERTopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m \u001b[43mBERTopic\u001b[49m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilingual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mfit_transform(docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BERTopic' is not defined"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(language=\"multilingual\")\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8918b73-bac1-4216-b2bd-d8cfae5dd725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
